{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "    Suppose an engineer is trying to solve a classification problem.\n",
    "    We managed to obtain accurate results for the training data, however we did not for the test\n",
    "    data.\n",
    "    Please list up the possible causes for this problem, and any proposed solutions that you may\n",
    "    have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Causes\n",
    "\n",
    "        The main objective behind building any model(be it for a classification problem) using a dataset certain size is to get an estimate (\"bigger picture\") of the complete domain (complete population) of which the available dataset is a subset of.\n",
    "       \n",
    "        After training the model and once we have goot fit over the testing data, it is recommended and crucial that we check its performance on some test data which the model never saw before, this step helps in estimating the model's capacity for generalization over the complete population.\n",
    "       \n",
    "        Now two scenarios can occur, \n",
    "        \n",
    "        1. The model meets our expection on the testing and we are ready to deploy.\n",
    "        \n",
    "        2. The model fails on the testing data, or previously 'unseen data'.\n",
    "        \n",
    "        In second case, we say that the model build is not genralising well and has become too specific to the training data that is model has picked up noise which is limited to the training data at hand and is hence not able to generalise, which is not desirable. We call this phenomemon 'Overfitting'. Technically speaking, the model has high variance.\n",
    "    \n",
    "        Now, possible reasons for a model to overfit would be :\n",
    "         \n",
    "         1. The test set consists of entities/labels which belong to a particular section of the population, which the model hasn't seen during the training. \n",
    "             \n",
    "             Eg. The train and test set split is such that all features belonging to a particular category are in test set only.\n",
    "         In simple terms, the train and test split is not random enough.\n",
    "         \n",
    "         2. The model has been optimized (too much) on the trainig data to such an extent that it has taken into account the patterns or noise which are very much specific to the training set and since the 'unseen' data would not be having any kind such noises it affects the model's performance on test data set.\n",
    "         \n",
    "         3. Insufficient training set size. As we build the model using the training data only and if the training data (here for classification problem) does not cover atleast few samples of different combinations of each of features for all the classes that we want predict then the model would fail to learn the general patterns which are necessary for its generalization over the complete population.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corresponding solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Perform a random shuffling of the data which we wish to model and only then proceed with the splitting it into training and testing set.\n",
    "\n",
    "    2. Our aim is not to learn or perfectly model the training data but to model the general population of which this training data is representative subset of. As the model trains for longer duration, the values of parameters converge to their optimal values with respect to the training data and hence training error keeps going down and so does the testing error for some time. But after some time as the model learns the noise which is very much specific to the training data, the training error goes down but the test error would increase which is not desirable. \n",
    "    \n",
    "        For this we should regularize the parameter that the model learns while maintaining the accuracy/performance of the model. We may want to :\n",
    "        a. Penalise large parameter values that model learns, so that no single alone feature would dictate complete behaviour of the model. \n",
    "        b. Decide on Early stopping criteria, so that we hit that sweet spot where both the training and testing error are minimum.\n",
    "    \n",
    "    3. Colleting more data. This may not be a very simple or straight forward process, when there is a certain cost associated with collecting more data. Alternative to this would be data augmentation, where small quantity of data of the required class is synthesized from the existing training data. One simple method for data augmentation would be copying the original data and introducing small random noise to it to mimic real would data entries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
