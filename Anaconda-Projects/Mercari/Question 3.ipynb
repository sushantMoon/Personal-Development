{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "        “Deep learning” is a term that is floated around a lot lately, but the idea itself has been around since the 80s or 90s. Why do you think that it has suddenly become such a hot topic again?\n",
    "        Please also give us your thoughts and opinion of deep learning in general to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To understand deep learning's popularity, I started with its history. I would be summarizing my findings here.\n",
    "\n",
    "        1. In 1940, Alan Turning proposed criteria for testing machine intelligence, this popularly known as Turing Test.\n",
    "        \n",
    "        2. In 1943, Walter Pitts and Warren McCulloch created the first mathematical model of a neural network. The basic unit of this network is called McCulloch–Pitts neuron. Later around 1960, Frank Rosenblatt proposed Perceptron which became the real precursor to the modern neural network. In 1969, Marvin Minsky pointed out though Perceptron could learn simple linear functions, it was impossible for it to learn non-linear functions like XOR. \n",
    "        \n",
    "        3. In 1960, Henry J. Kelly developed the basics of continuous Back Propagation Model using dynamic programming. In 1962, Stuart Dreyfus gave a simpler version based on chain rule only. In 1970, by Seppo Linnainmaa implemented the first backpropagation code.\n",
    "        \n",
    "        4. In 1986,  Geoff Hinton, along with  David Rumelhart and Ronald Williams showed that neural network with many hidden layers could be trained with a relatively simple procedure. This allowed neural networks to learn non-linear functions that Perceptron could not. It is also proved that it can learn any function in the famous theorem called Universal Approximation Theorem. In 1989, Yann LeCun builds the first convolutional neural network together with back propagation to recognize handwritten digits. At that time due to lack of computational power, it was not possible to scale to larger problems.  \n",
    "          \n",
    "        5. After 1999, the computational power became cheaper with the advent of faster GPU and processors. Also after 2001, Big Data term was coined based on the phenomenon observed from the increasing data volume as the data sources and types of data increased. These two factors played a major role in bringing Deep Learning to the masses.\n",
    "        \n",
    "        6. In 2006, Geoff Hinton introduced Unsupervised Pre-Training and Deep Belief Net which could be said as the first Deep Learning Algorithm. In recent years, many Deep Learning Algorithms have been developed. One such example would be AlexNet which won many competitions in 2011 and 2012. In 2012 Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton introduced ReLU(Rectified Linear Unit) Activation Function and Dropout method to reduce overfitting. This gave state of the art results.\n",
    "\n",
    "    So in short, though the core concepts were introduced before the '90s, a large amount of labeled data and cheaper computational power became available to train and test these different ideas very recently. \n",
    "\n",
    "    From its history, the factors that seem to have  revolutionized and in turn popularized deep learning are :\n",
    "        \n",
    "        1. Readily available large and high quality of labeled datasets required by deep learning methods to learn.\n",
    "        \n",
    "        2. Shifting from CPU based training methods to GPU based 'parallel computation' increasing the speed of training.\n",
    "        \n",
    "        3. Introduction to activation functions like ReLU which helped to backpropagate the errors better by solving vanishing gradient problem.\n",
    "        \n",
    "        4. The emergence of techniques like Dropout, Batch Normalization and Data Augmentation allowing us to train larger networks with less overfitting.\n",
    "        \n",
    "        5. Improved Optimizers like SGDm RMSprop, ADAM.\n",
    "\n",
    "    In my personal opinion,\n",
    "        * In the near future, deep learning would play a great role in human civilization with its innumerable applications for improving human life. \n",
    "        * With ever-increasing high-quality data and faster GPU's. The training and testing of larger deep learning models are getting easier leading to the rise of its application and research interest in aspiring researchers.\n",
    "        * I think every company that wants to survive and thrive in the future should embrace deep learning and make it an integral part of its products.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
