{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "## Source : \n",
    "        [Twitter Sentiment Analysis](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)\n",
    "## Resources :\n",
    "1. https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "2. https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/\n",
    "3. https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/\n",
    "4. https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "5. https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "6. http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "7. http://scikit-learn.org/stable/modules/pipeline.html\n",
    "8. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes\n",
    "from sklearn.metrics import precision_recall_fscore_support as pr\n",
    "from sklearn.linear_model import SGDClassifier  # Lineaer Classifier\n",
    "from sklearn.model_selection import GridSearchCV  # GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update my nltk data folder\n",
    "# nltk.download()\n",
    "\n",
    "pd.set_option('max_colwidth', 280)\n",
    "dataPath = \"E:\\\\GitHub\\\\Personal-Development\\\\Anaconda-Projects\\\\AnalyticsVidhya\\\\Twitter-Sentiment-Analysis\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps : Data Cleaning, Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataPath = dataPath + \"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(trainingDataPath,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe = { \n",
    "    \"'m\":\"am\",\"'d\":\"had\",\"'s\":\"is\",\"'ve\":\"have\",\"'ll\":\"will\",\"'re\":\"are\",\"'t\":\"not\"\n",
    "}\n",
    "# special cases are can't won't ain't\n",
    "# made using list under https://www.panopy.com/iphone/secret-ada/cracking-a-cipher.html\n",
    "\n",
    "# Stop words in english Language\n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)\n",
    "# Stemming Engine\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apostropheCleaner(x):\n",
    "    x = x.lower()\n",
    "    # Step 1 : removing apostrophe\n",
    "    words = x.split()\n",
    "    reformed = []\n",
    "    flag = False\n",
    "    for word in words:\n",
    "        flag = False\n",
    "        for k in apostrophe:\n",
    "            if k in word:\n",
    "                flag = True\n",
    "                if k == \"'t\":\n",
    "                    # for type : can't,won't,ain't\n",
    "                    if word == \"can't\":\n",
    "                        reformed.append(\"can not\")\n",
    "                    elif word == \"won't\":\n",
    "                        reformed.append(\"will not\")\n",
    "                    elif word == \"ain't\":\n",
    "                        reformed.append(\"is not\")\n",
    "                    else:\n",
    "                        reformed.append(word.split(\"'\")[0][:-1]+\" \"+apostrophe[k])\n",
    "                else:\n",
    "                    reformed.append(word.split(\"'\")[0]+\" \"+apostrophe[k])\n",
    "                break\n",
    "        if not flag:\n",
    "            reformed.append(word)\n",
    "    return \" \".join(reformed)\n",
    "\n",
    "def linkCleaner(x):\n",
    "    # Step 2: URL removal\n",
    "    # removing url,usernames and hashtag\n",
    "    return re.sub(r\"http\\S+|@\\S+\", \"\", x)\n",
    "    \n",
    "def hashtagExtract(x):    \n",
    "    return \" \".join(set(re.findall(r\"#(\\w+)\", x)))\n",
    "    \n",
    "def wordCleaner(x):\n",
    "    # remove the hadhtags that remained with the sentence\n",
    "    # x = re.sub(r\"#\\S+\", \"\", x)\n",
    "    \n",
    "    # # Step 3: Removing Punctuation\n",
    "    # tokens = word_tokenize(reform)\n",
    "    # words = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # # Step 4: Stemming Words\n",
    "    # stemmed = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # # Step 5: Removing Stop Words\n",
    "    # words = [w for w in stemmed if not w in stop_words]\n",
    "    \n",
    "    # Combining Step 3,4,5\n",
    "    tokens = word_tokenize(x)\n",
    "    words = []\n",
    "    for w in tokens:\n",
    "        if w.isalpha() and w not in stop_words:\n",
    "            # words.append(stemmer.stem(w))   //// ignoring stemming for now\n",
    "            words.append(w)\n",
    "    x = \" \".join(words)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962,)\n",
      "(31962,)\n"
     ]
    }
   ],
   "source": [
    "data['cleanedTweet'] = data['tweet'].transform(apostropheCleaner)\n",
    "\n",
    "data['cleanedTweet'] = data['cleanedTweet'].transform(linkCleaner)\n",
    "\n",
    "data['hastag'] = data['cleanedTweet'].transform(hashtagExtract)\n",
    "\n",
    "data['cleanedTweet'] = data['cleanedTweet'].transform(wordCleaner)\n",
    "\n",
    "# data[data['label'] == 1]\n",
    "\n",
    "features = data['cleanedTweet']\n",
    "label = data['label']\n",
    "\n",
    "print(features.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data using Sklearn Model Selection, this should give us better results for cross valication\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(features, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569,) (25569,) (6393,) (6393,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember \n",
    "    When data set is very skewed like in current scenario where there are very low number of `class 1` and very large number of `class 0`, \n",
    "    rather than calculating accuracy, we should be measuring the model using f1 Score, which is calculated using Precision and Recall\n",
    "### Precision :\n",
    "    * High Precision means low false positive\n",
    "    * It means low wrong prediction for class 0\n",
    "### Recall :\n",
    "    * High Recall means low false negative\n",
    "    * It means low wrong prediction for class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "text_clf_nb = Pipeline([\n",
    "                        ('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf_nb', MultinomialNB(alpha = 0.1))\n",
    "                       ])\n",
    "text_clf_nb = text_clf_nb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified as Class 1 : 267\n",
      "Total Actual Class 1 : 463\n",
      "Values for Naive Bayes :  Precision -> 0.895131086142  Recall-> 0.516198704104  F1score-> 0.654794520548\n"
     ]
    }
   ],
   "source": [
    "predicted_nb = text_clf_nb.predict(X_test)\n",
    "print(\"Classified as Class 1 :\", sum(predicted_nb))\n",
    "print(\"Total Actual Class 1 :\",sum(Y_test))\n",
    "# F1 Score, Precision, Recall\n",
    "Precision, Recall, F1score, Support = pr(Y_test, predicted_nb, average='binary', pos_label=1,beta=1.0)\n",
    "print(\"Values for Naive Bayes : \",\"Precision ->\",Precision,\" Recall->\",Recall,\" F1score->\",F1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier\n",
    "This Model can work as SVM too, with hinge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear CLassifer\n",
    "text_clf_linearClassifier = Pipeline([\n",
    "                                        ('vect', CountVectorizer()), \n",
    "                                        ('tfidf', TfidfTransformer()), \n",
    "                                        ('clf-svm', SGDClassifier(loss='perceptron', penalty='elasticnet', alpha=0.001, max_iter=1000, tol=0.001))\n",
    "                                    ])\n",
    "text_clf_linearClassifier = text_clf_linearClassifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified as Class 1 : 129\n",
      "Total Actual Class 1 : 463\n",
      "Values for Linear Classifier :  Precision -> 0.744186046512  Recall-> 0.207343412527  F1score-> 0.324324324324\n"
     ]
    }
   ],
   "source": [
    "predicted_linearClassifier = text_clf_linearClassifier.predict(X_test)\n",
    "print(\"Classified as Class 1 :\", sum(predicted_linearClassifier))\n",
    "print(\"Total Actual Class 1 :\",sum(Y_test))\n",
    "# F1 Score, Precision, Recall\n",
    "Precision, Recall, F1score, Support = pr(Y_test, predicted_linearClassifier, average='binary', pos_label=1,beta=1.0)\n",
    "print(\"Values for Linear Classifier : \",\"Precision ->\",Precision,\" Recall->\",Recall,\" F1score->\",F1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch \n",
    "It is used for finding the best fit of parameter for the defined estimator(predictor/model/classifier) from a range defined under `param_grid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch For Naive Bayes\n",
    "\n",
    "*   Defined to run on multi core\n",
    "*   Uses f1 scoring\n",
    "*   4-fold cross validation\n",
    "*   It is resistent to model failures (if it does) for any combination of parameters within the range it is searching for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the Parameters for setting up parameters for `param_grid`\n",
    "# text_clf_nb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "param_grid = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf_nb__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf_nb, param_grid, n_jobs=-1, cv=4, scoring='f1', error_score=np.NaN, refit=True)\n",
    "gs_clf = gs_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665723286674 {'clf_nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# print(gs_clf.best_score_,gs_clf.best_params_,gs_clf.best_estimator_)\n",
    "print(gs_clf.best_score_,gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified as Class 1 : 315\n",
      "Total Actual Class 1 : 463\n"
     ]
    }
   ],
   "source": [
    "# As refit is True, the estimators are already trained with the best possible parameters within the defined range \n",
    "# and we can use predict fuction directly \n",
    "predicted_gs_clf = gs_clf.predict(X_test)\n",
    "print(\"Classified as Class 1 :\", sum(predicted_gs_clf))\n",
    "print(\"Total Actual Class 1 :\",sum(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for Linear Classifier :  Precision -> 0.898412698413  Recall-> 0.611231101512  F1score-> 0.727506426735\n",
      "high precision means low false positive(means low wrong prediction for class 0), high recall means low false negative(means low wrong prediction for class 1)\n"
     ]
    }
   ],
   "source": [
    "# F1 Score, Precision, Recall\n",
    "Precision, Recall, F1score, Support = pr(Y_test, predicted_gs_clf, average='binary', pos_label=1,beta=1.0)\n",
    "print(\"Values for Linear Classifier : \",\"Precision ->\",Precision,\" Recall->\",Recall,\" F1score->\",F1score)\n",
    "print(\"high precision means low false positive(means low wrong prediction for class 0), high recall means low false negative(means low wrong prediction for class 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf_linearClassifier.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submission Test Tweets Prep \n",
    "# submissionDataPath = dataPath + \"test.csv\"\n",
    "# submissionData = pd.read_csv(submissionDataPath,encoding='utf-8')\n",
    "\n",
    "\n",
    "# submissionData['cleanedTweet'] = submissionData['tweet'].transform(apostropheCleaner)\n",
    "# submissionData['cleanedTweet'] = submissionData['cleanedTweet'].transform(linkCleaner)\n",
    "# submissionData['hastag'] = submissionData['cleanedTweet'].transform(hashtagExtract)\n",
    "# submissionData['cleanedTweet'] = submissionData['cleanedTweet'].transform(wordCleaner)\n",
    "\n",
    "\n",
    "# submissionData['label'] =  gs_clf.predict(submissionData['cleanedTweet'])\n",
    "# submission = submissionData[[\"id\",\"label\"]].set_index(\"id\")\n",
    "\n",
    "# # submissionData.describe()\n",
    "# submission.to_csv(dataPath+\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
